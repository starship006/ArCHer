defaults:
  - default
  - addie_filepaths
  - _self_

# checkpoint

# env
env_name: twenty_questions
simplified: False
bsize: 32 # number of rollouts collected at once, ONLY IN 20Q ENV RIGHT NOW

# model
agent_type: 'archer_llm'
critic_lm: "roberta-base" 
max_new_tokens: 64
use_lora: True
eos_str: null
quantize: False

capacity: 100000 #replay buffer size
rollout_size: 32 #number of rollout trajectories for each update
eval_size: 32 #number of trajectories for evaluation
batch_size: 32 # from 32 since 32 sometimes hits cuda limit. so does 28.s
update_batch_size: 12 # on h100 node: 12 works # 16 doesn't #8 works # from 32
actor_epochs: 16 #number of epochs for the actor each iteration

iterations: 2000 #total number of iterations
epochs: 12  #20 #number of epochs for the critic each iteration

warmup_iter: 20 #number of iterations without updating the policy
grad_accum_steps: 8 #16
do_sample: True
temperature: 1.0
critic_lr: 1e-5
lm_lr: 2e-6
env_idx: null #set to null if don't want to reset to a specific environment
gamma: 0.95 #discount factor
tau: 0.1 #soft update parameter
max_grad_norm: 2.0

# wandb logging
use_wandb: True
project_name: 'llm_rl_20qsubset'
run_name: 'archer-acc-llm-1e-6-subset'

use_bfloat16: True
save_freq: 2
eval_freq: 4